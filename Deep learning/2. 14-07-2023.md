
# ReLU

(jajaja llegué media hora tarde, creo que esta es una función de activación, creo que está hablando de funciones de activación)

Es un modelo que elimina lineridad pero existe el riesgo que se muera la nuerona, en si eso no representa un problema pero no lo ideal que esto este sucediendo esto a cada rato. En todo caso, para reducir esto se usa Leaky ReLU function. 


Con las funciones de activación, hay que elegir la que más rápido se parezca a la función que quiera imitar. En si, la elección de la función de activación va orientado al problema seleccionado. 


Backpropagation es lo que hace fuerte a las redes neuronales. 
- Tiene la función de perdida, como el MSE
	- El objetivo de esto es medir la diferencia entre lo esperado y lo que dijo la red neuronal. 
	- 


# Problemas y sus funciones de pérdida

## Regresión
Podemos usar MSE. 

$MSE = 1/n \sum_{i=1}^n{y^i - y_rî}$
**Pros**
Penaliza los errores más grandes. 
Convexa: Fácil de encontrar mínimo global. 
**Cons:**
Sensible a outlayers. Solución MAE. Es mejor usar el valor absoluto, pero esto hace que la función ya no sea derivable. 


Entonces, alguien se inventó otra métrica llamada: *Huber Loss*


La solución para cuando hay datos atípicos


## Binary classification

Cálculo de error clasificando dos categorias. 
### Log loss

$log_loss = \frac{1}{N}\sum_{i=1}^N{-(y_i \cdot log(p_i) + (1-y_i)\cdot log(1-p_i))}$
pros: Fácil de implmentar y optimizar
cons: sensible a clases desbalanceadas

Para nivelar: 
- Resamplear
- Nivelaciones sintéticas??

### Hinge loss

También se utiliza para SVM y otros modelos de "hiperplanos" (o algo así)?

 $l = max(0, 1-y^i(x^i-b))$

## Multiclass Classification
### Multiclass cross entropy

$L(y_r, y)= -\sum_k^K{y^klog(y_r^k)}$

Mismo conceptro que en Binary Crsoo Entropy, pero con K clases en lugar de solo 2. 

Pros: fácil de implementar y optimizar. 
Const. Sensiblea clases desbalanceada


# Optimizadores

## Gradiente de descenso 

**Meta**:
Encontrar el paso hacia donde la curva descenso de manera más pronunciada. 
**¿Cómo?**
Haciendo modificaciones a los parámetros de la NN (pesos y segos) para minimizar el valor de la función de pérdida. 

$w := w - n\Delta Q_i(w)$
# Otras notas

Dice que hay muchos valores que hay que definir desde el inicio (hiperparameetros) que prácticamente se calculan a prueba y error. ¿?