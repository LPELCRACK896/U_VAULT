

# Introducción 

Explica el programa del curso y así. 

# Modelos de clasificación de los sistemas paralelos

## Agenda 

- Arquitectura *Von Neuman* y sus modificaciones. 
- Hardware paralelo. 
- Clasificación de sistema de cómputo. 
- Niveles de paralelismo. 


## Inicio

Pregunta conceptos generales de que es una computadora, que es paralelismo, cual es la limitación de la capacidad de procesadores (*Ley de Moore*). Menciona la comparación de la Ley de Moore con la realidad, dado que existe estándares con la frecuencia derivado de las temperaturas y los procesadores multinúcleos. 




## Computadoras modernas

### Ley de Moore
La Ley de Moore es una observación que se hizo en 1965 por Gordon Moore, co-fundador de Intel. Esta "ley" sostiene que la cantidad de transistores en un microprocesador (una medida de la potencia informática) se duplicaría aproximadamente cada dos años. Durante varias décadas, la Ley de Moore ha sido una guía razonablemente precisa para el desarrollo de la tecnología de los semiconductores.

No obstante, al día de hoy, en 2023, la Ley de Moore ha encontrado numerosos desafíos y algunos argumentan que ya no es válida.

Aquí te presento algunos de los principales obstáculos:

**1. Límites Físicos:** A medida que los transistores se vuelven más pequeños, llegan a un punto en que solo constan de un puñado de átomos. A esta escala, la física cuántica empieza a tener un impacto significativo. Los fenómenos como la interferencia cuántica y el efecto túnel pueden causar que los transistores no funcionen de manera confiable.

**2. Problemas de Disipación de Calor:** A medida que metemos más transistores en un espacio más pequeño, la cantidad de calor que generan aumenta. Este calor debe ser disipado para evitar que los transistores se sobrecalienten y fallen. Pero a medida que seguimos aumentando la densidad de los transistores, se vuelve cada vez más difícil disipar este calor de manera efectiva.

**3. Costos de Fabricación:** A medida que la tecnología de semiconductores se vuelve más avanzada, también se vuelve más costosa. Los costos de investigación y desarrollo, así como los costos de construcción de nuevas fábricas de semiconductores que pueden fabricar chips con estas tecnologías más avanzadas, están aumentando. Estos crecientes costos pueden limitar la velocidad a la que podemos seguir mejorando la tecnología de los semiconductores.

**4. Consumo de Energía:** A medida que aumenta la densidad de transistores, también aumenta el consumo de energía, lo que puede ser un problema, especialmente para dispositivos móviles que dependen de baterías.

A pesar de estos desafíos, la industria de semiconductores sigue innovando, buscando nuevas formas de mejorar la potencia de cálculo. Esto incluye nuevas arquitecturas de chips, como los chips de múltiples núcleos y los chips 3D, así como tecnologías emergentes como los computadoras cuánticas y la computación neuromórfica. Aunque la Ley de Moore tal como la conocemos puede estar llegando a su fin, la innovación en la tecnología informática ciertamente no se detiene.


### Paralelismo
A partir del 2002 se adoptan estrategia paralelas para mejorar el rendimiento. Afectando igualmente la validez de la ley de moore que en un principio no contemplaba este factor. Hoy día, toda computadora moderna es paralela. 

## Arquitectura de Von Neumann

[[Z Resumen Libro An Introduction to PARALLEL PROGRAMMING by Peter Pacheco#La arquitectura Von Neumann|Modelo Von Neumann según el libro]]


En un sistama deben existir los siguiente componentes: 
- Unidad de procesamiento (UP): 
	- CPU/Core
- Sistema de Interconexión
- Memoria primaria 

![[IMG Arquitectura.png]]
###  Ventajas v Desventajas

|Ventajas | Desventajas (BN) |
|----------|--------|
|Igual forma de acceder a datos en I/O y memoria| Tamaño y características del bus de interconexión|
|Simplicidad el diseño de las unidades de control|Reescritura de instrucciones|
|Igual forma de recuperar datos e instrucciones | Procesamiento secuencial (PC - program counter) |

## Modificaciones Von Neumann

### Jamboard
- Arquitectura Harvard − Canales separados de
acceso a memoria de datos y de instrucciones.
- Caché − Jerarquía de memorias para almacenaje temporal.
- Memoria virtual − mapeo de memoria primaria a medios de almacenaje.
- Paralelismo de HW − múltiples recursos y estrategias para procesamiento paralelo a nivel de HW (pipeline, multithreading, multiple issue)
#### Caché
Memoria en mismo chip (o fuera con una interconexión de alta velocidad). Con su propia jerarquía interna L1, L2, L3. 

Aprovecha la localidad
- Espacial - datos contiguos
- Temporal - datos con mayor probabilidad inmediata de ser requerida. 

|- | Mayor velocidad | Velodidad media | Velocidad baja|
|----|-|- | -|
|Espacio mínimo | L1 | - | - | 
|Espacio medio | - | L2 | -| 
|Espacio máximo | - | - | L3 |

##### Consideraciones
Con varios niveles de Caché o una caché compartida por varias [[U/Computación paralela/Glosario#UP|UP]]'s. 
Me quedé en la slide 14. 